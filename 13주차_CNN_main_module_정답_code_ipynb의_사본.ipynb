{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maminseo/minseo/blob/main/13%EC%A3%BC%EC%B0%A8_CNN_main_module_%EC%A0%95%EB%8B%B5_code_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8CW-ldez0bV"
      },
      "source": [
        "## 패키지 선언"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8y9SBNBvz989"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision.datasets as dataset\n",
        "import torchvision.transforms as transform\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIZvkTTVz_We"
      },
      "source": [
        "## Dataset 선언"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22hst1Gx0DPK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0908bbff-2bb4-4d5e-b526-6278c03f1afb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 35.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Training dataset 다운로드\n",
        "cifar10_train = dataset.CIFAR10(root = \"./\", # 데이터셋을 저장할 위치\n",
        "                            train = True,\n",
        "                            transform = transform.ToTensor(),\n",
        "                            download = True)\n",
        "# Testing dataset 다운로드\n",
        "cifar10_test = dataset.CIFAR10(root = \"./\",\n",
        "                            train = False,\n",
        "                            transform = transform.ToTensor(),\n",
        "                            download = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARS1c9CD0ElD"
      },
      "source": [
        "## CIFAR10 데이터셋 형상 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZdigwTg0GnB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "4cc016b5-9c3b-4b29-a4bc-cdd67249fb3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000\n",
            "torch.Size([3, 32, 32])\n",
            "9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "print(len(cifar10_train))     # training dataset 개수 확인\n",
        "\n",
        "first_data = cifar10_train[1]\n",
        "print(first_data[0].shape)  # 두번째 data의 형상 확인\n",
        "print(first_data[1])        # 두번째 data의 정답 확인\n",
        "\n",
        "\n",
        "plt.imshow(first_data[0].permute(1, 2, 0))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOCujlwh0TJw"
      },
      "source": [
        "## VGG Net 모델 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3LB6Iv-0U56"
      },
      "outputs": [],
      "source": [
        "class VGG (nn.Module):\n",
        "  def __init__ (self):\n",
        "    super(VGG, self).__init__()\n",
        "\n",
        "    self.conv1_1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)    # Convolution: [3x3x3] x 16, s1, p1\n",
        "    self.conv1_2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)   # Convolution: [3x3x16] x 32, s1, p1\n",
        "\n",
        "    self.conv2_1 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)   # Convolution: [3x3x32] x 32, s1, p1\n",
        "    self.conv2_2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)   # Convolution: [3x3x64] x 64, s1, p1\n",
        "\n",
        "    self.conv3_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)  # Convolution: [3x3x64] x 128, s1, p1\n",
        "    self.conv3_2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1) # Convolution: [3x3x128] x 256, s1, p1\n",
        "\n",
        "    self.fc1 = nn.Linear(4096, 512)   # Fully connected layer: 4096 x 512\n",
        "    self.fc2 = nn.Linear(512, 256)    # Fully connected layer: 512 x 256\n",
        "    self.fc3 = nn.Linear(256, 10)     # Fully connected layer: 256 x 10\n",
        "\n",
        "    # 파라미터를 가지지 않은 layer는 한 번만 선언해도 문제 없음\n",
        "    self.relu = nn.ReLU()\n",
        "    self.avgPool2d = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    # convolution layers\n",
        "    out = self.relu(self.conv1_1(x))\n",
        "    out = self.relu(self.conv1_2(out))\n",
        "    out = self.avgPool2d(out)\n",
        "\n",
        "    out = self.relu(self.conv2_1(out))\n",
        "    out = self.relu(self.conv2_2(out))\n",
        "    out = self.avgPool2d(out)\n",
        "\n",
        "    out = self.relu(self.conv3_1(out))\n",
        "    out = self.relu(self.conv3_2(out))\n",
        "    out = self.avgPool2d(out)\n",
        "\n",
        "    # 평탄화\n",
        "    out = out.reshape(-1, 4096)\n",
        "\n",
        "    # fully connected layers\n",
        "    out = self.relu(self.fc1(out))\n",
        "    out = self.relu(self.fc2(out))\n",
        "    out = self.fc3(out)\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltAhcjO60b0x"
      },
      "source": [
        "## Hyper-parameters 지정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1INfQRt0eJm"
      },
      "outputs": [],
      "source": [
        "\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "training_epochs = 20\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "network = VGG()\n",
        "optimizer = torch.optim.SGD(network.parameters(), lr = learning_rate)\n",
        "data_loader = DataLoader(dataset=cifar10_train,\n",
        "                         batch_size=batch_size,\n",
        "                         shuffle=True,\n",
        "                         drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfy95Byn0f1E"
      },
      "source": [
        "## 학습을 위한 반복문 선언"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEngtl0a0hFs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9956c8c0-b27a-401b-f58b-c003b0893fc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1730609884.py:24: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
            "Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)\n",
            "  print('Epoch: %d Loss = %f'%(epoch+1, avg_cost))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 Loss = 2.302973\n",
            "Epoch: 2 Loss = 2.302765\n",
            "Epoch: 3 Loss = 2.291579\n",
            "Epoch: 4 Loss = 2.303059\n",
            "Epoch: 5 Loss = 2.302958\n",
            "Epoch: 6 Loss = 2.282456\n",
            "Epoch: 7 Loss = 2.054112\n",
            "Epoch: 8 Loss = 1.853630\n",
            "Epoch: 9 Loss = 1.738861\n",
            "Epoch: 10 Loss = 1.625721\n",
            "Epoch: 11 Loss = 1.439021\n",
            "Epoch: 12 Loss = 1.302568\n",
            "Epoch: 13 Loss = 1.189429\n",
            "Epoch: 14 Loss = 1.088422\n",
            "Epoch: 15 Loss = 0.983315\n",
            "Epoch: 16 Loss = 0.876823\n",
            "Epoch: 17 Loss = 0.762764\n",
            "Epoch: 18 Loss = 0.647175\n",
            "Epoch: 19 Loss = 0.523265\n",
            "Epoch: 20 Loss = 0.422677\n",
            "Learning finished\n"
          ]
        }
      ],
      "source": [
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda:0'\n",
        "\n",
        "network = network.to(device)\n",
        "for epoch in range(training_epochs):\n",
        "  avg_cost = 0\n",
        "  total_batch = len(data_loader)\n",
        "\n",
        "  for img, label in data_loader:\n",
        "\n",
        "    img = img.to(device)\n",
        "    label = label.to(device)\n",
        "\n",
        "    pred = network(img)\n",
        "\n",
        "    loss = loss_function(pred, label)\n",
        "    optimizer.zero_grad() # gradient 초기화\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    avg_cost += loss / total_batch\n",
        "\n",
        "  print('Epoch: %d Loss = %f'%(epoch+1, avg_cost))\n",
        "\n",
        "print('Learning finished')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UeKYHAz1NdM"
      },
      "source": [
        "## 학습이 완료된 모델을 이용해 정답률 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AoZ9Dmh1Pxj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47e25e11-449d-4f9f-ac52-b003f63c6e32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5997999906539917\n"
          ]
        }
      ],
      "source": [
        "network = network.to('cpu')\n",
        "with torch.no_grad(): # test에서는 기울기 계산 제외\n",
        "\n",
        "  img_test = torch.tensor(np.transpose(cifar10_test.data, (0, 3, 1, 2)))/255.\n",
        "  label_test = torch.tensor(cifar10_test.targets)\n",
        "\n",
        "  prediction = network(img_test) # 전체 test data를 한번에 계산\n",
        "\n",
        "  correct_prediction = torch.argmax(prediction, 1) == label_test\n",
        "  accuracy = correct_prediction.float().mean()\n",
        "  print('Accuracy:', accuracy.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZmsmIJpC4Xy"
      },
      "source": [
        "## -----------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUYfK3LVEDvm"
      },
      "source": [
        "## Model architecture 구성 실습 (ResNet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VotA21Y3EKLh"
      },
      "source": [
        "## 모델 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyEDRgOoELTR"
      },
      "outputs": [],
      "source": [
        "class VGG_SKIP (nn.Module):\n",
        "  def __init__ (self):\n",
        "    super(VGG_SKIP, self).__init__()\n",
        "\n",
        "    self.conv1_1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)    # Convolution: [3x3x3] x 16, s1, p1\n",
        "    self.conv1_2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)   # Convolution: [3x3x16] x 32, s1, p1\n",
        "\n",
        "    self.conv2_1 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)   # Convolution: [3x3x32] x 32, s1, p1\n",
        "    self.conv2_2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)   # Convolution: [3x3x64] x 64, s1, p1\n",
        "\n",
        "    self.conv3_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)  # Convolution: [3x3x64] x 128, s1, p1\n",
        "    self.conv3_2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1) # Convolution: [3x3x128] x 256, s1, p1\n",
        "\n",
        "    # Skip connection을 위한 convolution layer\n",
        "    self.conv_skip1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
        "    self.conv_skip2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "    self.conv_skip3 = nn.Conv2d(in_channels=64, out_channels=256, kernel_size=3, padding=1)\n",
        "\n",
        "    self.fc1 = nn.Linear(4096, 512)   # Fully connected layer: 4096 x 512\n",
        "    self.fc2 = nn.Linear(512, 256)    # Fully connected layer: 512 x 256\n",
        "    self.fc3 = nn.Linear(256, 10)     # Fully connected layer: 256 x 10\n",
        "\n",
        "    # 파라미터를 가지지 않은 layer는 한 번만 선언해도 문제 없음\n",
        "    self.relu = nn.ReLU()\n",
        "    self.avgPool2d = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # convolution layers\n",
        "    # Skip 입력을 위한 input 저장\n",
        "    input_feature = x\n",
        "    out = self.relu(self.conv1_1(x))\n",
        "    out = self.relu(self.conv1_2(out))\n",
        "\n",
        "    # Skip 입력을 위한 convolution layer 적용\n",
        "    input_skip =self.relu(self.conv_skip1(input_feature))\n",
        "    # Skip connection 적용\n",
        "    out =torch.add(out, input_skip)\n",
        "    out1 = self.avgPool2d(out)\n",
        "\n",
        "    # Skip 입력을 위한 input 저장\n",
        "    input_feature = out1\n",
        "\n",
        "    out = self.relu(self.conv2_1(out1))\n",
        "    out = self.relu(self.conv2_2(out))\n",
        "\n",
        "    # Skip 입력을 위한 convolution layer 적용\n",
        "    input_skip =self.relu(self.conv_skip2(input_feature))\n",
        "    # Skip connection 적용\n",
        "    out =torch.add(out, input_skip)\n",
        "\n",
        "    out2 = self.avgPool2d(out)\n",
        "\n",
        "    # Skip 입력을 위한 input 저장\n",
        "    input_feature = out2\n",
        "\n",
        "    out = self.relu(self.conv3_1(out2))\n",
        "    out = self.relu(self.conv3_2(out))\n",
        "\n",
        "    # Skip 입력을 위한 convolution layer 적용\n",
        "    input_skip =self.relu(self.conv_skip3(input_feature))\n",
        "    # Skip connection 적용\n",
        "    out =torch.add(out, input_skip)\n",
        "\n",
        "    out = self.avgPool2d(out)\n",
        "\n",
        "    # 평탄화\n",
        "    out = out.reshape(-1, 4096)\n",
        "\n",
        "    # fully connected layers\n",
        "    out = self.relu(self.fc1(out))\n",
        "    out = self.relu(self.fc2(out))\n",
        "    out = self.fc3(out)\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFujRpv4ESAR"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6fhVYoiES7d"
      },
      "outputs": [],
      "source": [
        "# Hyper-parameters 지정\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "training_epochs = 20\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "network = VGG_SKIP()\n",
        "optimizer = torch.optim.SGD(network.parameters(), lr = learning_rate)\n",
        "data_loader = DataLoader(dataset=cifar10_train,\n",
        "                         batch_size=batch_size,\n",
        "                         shuffle=True,\n",
        "                         drop_last=True)\n",
        "\n",
        "# 학습을 위한 반복문 진행\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda:0'\n",
        "\n",
        "network = network.to(device)\n",
        "for epoch in range(training_epochs):\n",
        "  avg_cost = 0\n",
        "  total_batch = len(data_loader)\n",
        "\n",
        "  for img, label in data_loader:\n",
        "\n",
        "    img = img.to(device)\n",
        "    label = label.to(device)\n",
        "\n",
        "    pred = network(img)\n",
        "\n",
        "    loss = loss_function(pred, label)\n",
        "    optimizer.zero_grad() # gradient 초기화\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    avg_cost += loss / total_batch\n",
        "\n",
        "  print('Epoch: %d Loss = %f'%(epoch+1, avg_cost))\n",
        "\n",
        "print('Learning finished')\n",
        "\n",
        "# 정답률 확인\n",
        "network = network.to('cpu')\n",
        "with torch.no_grad(): # test에서는 기울기 계산 제외\n",
        "\n",
        "  img_test = torch.tensor(np.transpose(cifar10_test.data, (0, 3, 1, 2)))/255.\n",
        "  label_test = torch.tensor(cifar10_test.targets)\n",
        "\n",
        "  prediction = network(img_test) # 전체 test data를 한번에 계산\n",
        "\n",
        "  correct_prediction = torch.argmax(prediction, 1) == label_test\n",
        "  accuracy = correct_prediction.float().mean()\n",
        "  print('Accuracy:', accuracy.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abZJhykEC4Xz"
      },
      "source": [
        "## -----------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhA7jSSSC4Xz"
      },
      "source": [
        "## Model architecture 구성 실습 (DenseNet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9VWgCDiC4Xz"
      },
      "source": [
        "## 모델 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5l9ZARdLC4Xz"
      },
      "outputs": [],
      "source": [
        "class VGG_DENSE(nn.Module):\n",
        "  def __init__ (self):\n",
        "    super(VGG_DENSE, self).__init__()\n",
        "\n",
        "    self.conv1_1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)    # Convolution: [3x3x3] x 16, s1, p1\n",
        "    self.conv1_2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)   # Convolution: [3x3x16] x 32, s1, p1\n",
        "\n",
        "    self.conv2_1 = nn.Conv2d(in_channels=35, out_channels=32, kernel_size=3, padding=1)# Convolution: [3x3x35] x 32, s1, p1\n",
        "    self.conv2_2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)   # Convolution: [3x3x64] x 64, s1, p1\n",
        "\n",
        "    self.conv3_1 = nn.Conv2d(in_channels=99, out_channels=128, kernel_size=3, padding=1)# Convolution: [3x3x99] x 128, s1, p1\n",
        "    self.conv3_2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1) # Convolution: [3x3x128] x 256, s1, p1\n",
        "\n",
        "    self.fc1 = nn.Linear(4096, 512)   # Fully connected layer: 4096 x 512\n",
        "    self.fc2 = nn.Linear(512, 256)    # Fully connected layer: 512 x 256\n",
        "    self.fc3 = nn.Linear(256, 10)     # Fully connected layer: 256 x 10\n",
        "\n",
        "    # 파라미터를 가지지 않은 layer는 한 번만 선언해도 문제 없음\n",
        "    self.relu = nn.ReLU()\n",
        "    self.avgPool2d = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    # convolution layers\n",
        "    out = self.relu(self.conv1_1(x))\n",
        "    out1 = self.relu(self.conv1_2(out))\n",
        "\n",
        "    # Dense connection 적용\n",
        "    out1 = torch.cat([x,out1], dim=1)\n",
        "\n",
        "    out1 = self.avgPool2d(out1)\n",
        "\n",
        "    out = self.relu(self.conv2_1(out1))\n",
        "    out2 = self.relu(self.conv2_2(out))\n",
        "\n",
        "    # Dense connection 적용\n",
        "    out2 = torch.cat([out1, out2], dim=1)\n",
        "\n",
        "    out2 = self.avgPool2d(out2)\n",
        "\n",
        "    out = self.relu(self.conv3_1(out2))\n",
        "    out = self.relu(self.conv3_2(out))\n",
        "    out = self.avgPool2d(out)\n",
        "\n",
        "    # 평탄화\n",
        "    out = out.reshape(-1, 4096)\n",
        "\n",
        "    # fully connected layers\n",
        "    out = self.relu(self.fc1(out))\n",
        "    out = self.relu(self.fc2(out))\n",
        "    out = self.fc3(out)\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vmkTfuqC4Xz"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QM4gUctSC4Xz"
      },
      "outputs": [],
      "source": [
        "# Hyper-parameters 지정\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "training_epochs = 20\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "network = VGG_DENSE()\n",
        "optimizer = torch.optim.SGD(network.parameters(), lr = learning_rate)\n",
        "data_loader = DataLoader(dataset=cifar10_train,\n",
        "                         batch_size=batch_size,\n",
        "                         shuffle=True,\n",
        "                         drop_last=True)\n",
        "\n",
        "# 학습을 위한 반복문 진행\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda:0'\n",
        "\n",
        "network = network.to(device)\n",
        "for epoch in range(training_epochs):\n",
        "  avg_cost = 0\n",
        "  total_batch = len(data_loader)\n",
        "\n",
        "  for img, label in data_loader:\n",
        "\n",
        "    img = img.to(device)\n",
        "    label = label.to(device)\n",
        "\n",
        "    pred = network(img)\n",
        "\n",
        "    loss = loss_function(pred, label)\n",
        "    optimizer.zero_grad() # gradient 초기화\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    avg_cost += loss / total_batch\n",
        "\n",
        "  print('Epoch: %d Loss = %f'%(epoch+1, avg_cost))\n",
        "\n",
        "print('Learning finished')\n",
        "\n",
        "# 정답률 확인\n",
        "network = network.to('cpu')\n",
        "with torch.no_grad(): # test에서는 기울기 계산 제외\n",
        "\n",
        "  img_test = torch.tensor(np.transpose(cifar10_test.data, (0, 3, 1, 2)))/255.\n",
        "  label_test = torch.tensor(cifar10_test.targets)\n",
        "\n",
        "  prediction = network(img_test) # 전체 test data를 한번에 계산\n",
        "\n",
        "  correct_prediction = torch.argmax(prediction, 1) == label_test\n",
        "  accuracy = correct_prediction.float().mean()\n",
        "  print('Accuracy:', accuracy.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tcw3JPSIC4Xz"
      },
      "source": [
        "## -----------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_NvIQOoC4Xz"
      },
      "source": [
        "## Model architecture 구성 실습 (SENet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sb7wm5vC4Xz"
      },
      "source": [
        "## 모델 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z772aqcGC4Xz"
      },
      "outputs": [],
      "source": [
        "class VGG_CA (nn.Module):\n",
        "  def __init__ (self):\n",
        "    super(VGG_CA, self).__init__()\n",
        "\n",
        "    self.conv1_1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)    # Convolution: [3x3x3] x 16, s1, p1\n",
        "    self.conv1_2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)   # Convolution: [3x3x16] x 32, s1, p1\n",
        "\n",
        "    self.conv2_1 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1)   # Convolution: [3x3x32] x 32, s1, p1\n",
        "    self.conv2_2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)   # Convolution: [3x3x64] x 64, s1, p1\n",
        "\n",
        "    self.conv3_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)  # Convolution: [3x3x64] x 128, s1, p1\n",
        "    self.conv3_2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1) # Convolution: [3x3x128] x 256, s1, p1\n",
        "\n",
        "    self.fc1 = nn.Linear(4096, 512)   # Fully connected layer: 4096 x 512\n",
        "    self.fc2 = nn.Linear(512, 256)    # Fully connected layer: 512 x 256\n",
        "    self.fc3 = nn.Linear(256, 10)     # Fully connected layer: 256 x 10\n",
        "\n",
        "    # Channel Attention parameters\n",
        "    self.adaptiveAvgPool2d = nn.AdaptiveAvgPool2d((1,1))\n",
        "    self.caconv1 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=1)\n",
        "    self.caconv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    # 파라미터를 가지지 않은 layer는 한 번만 선언해도 문제 없음\n",
        "    self.relu = nn.ReLU()\n",
        "    self.avgPool2d = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    # convolution layers\n",
        "    out = self.relu(self.conv1_1(x))\n",
        "    out = self.relu(self.conv1_2(out))\n",
        "    out = self.avgPool2d(out)\n",
        "\n",
        "    out = self.relu(self.conv2_1(out))\n",
        "    out_CA = self.relu(self.conv2_2(out))\n",
        "\n",
        "    # Channal attention 적용\n",
        "    out = self.adaptiveAvgPool2d(out_CA)\n",
        "    out = self.relu(self.caconv1(out))\n",
        "    out = self.sigmoid(self.caconv2(out))\n",
        "    out = out*out_CA\n",
        "\n",
        "    out = self.avgPool2d(out)\n",
        "\n",
        "    out = self.relu(self.conv3_1(out))\n",
        "    out = self.relu(self.conv3_2(out))\n",
        "    out = self.avgPool2d(out)\n",
        "\n",
        "    # 평탄화\n",
        "    out = out.reshape(-1, 4096)\n",
        "\n",
        "    # fully connected layers\n",
        "    out = self.relu(self.fc1(out))\n",
        "    out = self.relu(self.fc2(out))\n",
        "    out = self.fc3(out)\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2nqcl48C4Xz"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7S52rpsnC4Xz"
      },
      "outputs": [],
      "source": [
        "# Hyper-parameters 지정\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "training_epochs = 20\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "network = VGG_CA()\n",
        "optimizer = torch.optim.SGD(network.parameters(), lr = learning_rate)\n",
        "data_loader = DataLoader(dataset=cifar10_train,\n",
        "                         batch_size=batch_size,\n",
        "                         shuffle=True,\n",
        "                         drop_last=True)\n",
        "\n",
        "# 학습을 위한 반복문 진행\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda:0'\n",
        "\n",
        "network = network.to(device)\n",
        "for epoch in range(training_epochs):\n",
        "  avg_cost = 0\n",
        "  total_batch = len(data_loader)\n",
        "\n",
        "  for img, label in data_loader:\n",
        "\n",
        "    img = img.to(device)\n",
        "    label = label.to(device)\n",
        "\n",
        "    pred = network(img)\n",
        "\n",
        "    loss = loss_function(pred, label)\n",
        "    optimizer.zero_grad() # gradient 초기화\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    avg_cost += loss / total_batch\n",
        "\n",
        "  print('Epoch: %d Loss = %f'%(epoch+1, avg_cost))\n",
        "\n",
        "print('Learning finished')\n",
        "\n",
        "# 정답률 확인\n",
        "network = network.to('cpu')\n",
        "with torch.no_grad(): # test에서는 기울기 계산 제외\n",
        "\n",
        "  img_test = torch.tensor(np.transpose(cifar10_test.data, (0, 3, 1, 2)))/255.\n",
        "  label_test = torch.tensor(cifar10_test.targets)\n",
        "\n",
        "  prediction = network(img_test) # 전체 test data를 한번에 계산\n",
        "\n",
        "  correct_prediction = torch.argmax(prediction, 1) == label_test\n",
        "  accuracy = correct_prediction.float().mean()\n",
        "  print('Accuracy:', accuracy.item())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}